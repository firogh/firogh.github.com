# Linus Torvalds: looking back, looking forward
Most years, Linus Torvalds comes to Australia. He apparently likes the place, so the creator of the Linux kernel makes his way to the Australian national Linux conference in January.
He's been doing this since 2003, when the affable James Bromberger and Tony Breeds (corrected) organised the LCA in Perth, only missing out in 2010 when the event moved across the Tasman to Wellington.
Torvalds is an excellent subject for an interview; he never evades a question, not even if he has a single word to offer as reply. And despite claiming to have a big ego, he is indeed very approachable.
He couldn't sit down for an interview as he was leaving the conference the morning after I approached him; hence this was done by email.
iTWire: It's been nearly 20 years now since that now-famous message posted to comp.os.minix announced the arrival of something that has grown beyond anyone's imagination. Have there been any occasions when you've thought of walking away from it?
Linus Torvalds: Walking away? No. There have certainly been times of frustration, and times when I've really wanted to take a break, but quite frankly, even then it's been "I need to get away from this for a few hours (or days) to relax and do something else" rather than anything more than that.
And I can't imagine doing it now either. I'd just get really bored very quickly. There have been stressful times, but in hindsight even the stressful times have usually been things that really were worthwhile in the end. And interestingly (and maybe not all that surprisingly, but it always took me by surprise every time it happened anyway), most of the ones that have been the most stressful have been not really about technical issues. They've always fundamentally been about development model. So there's been several times when the way I did something just didn't work out well as the kernel project grew, and I (and others) needed to change how we worked in Linus Torvaldsorder to streamline the process and make it work better in the face of many new developers.
As an example, we had the multi-year release cycles, which really led to a lot of pain and caused all the distributions to have to back-port a lot of code to the "stable" version, which got increasingly hard as the development tree diverged quite radically over the years. And people still remember that model, and the whole "even release numbers" (for stable, ie 2.4.x) vs "odd release numbers" (for development, ie 2.5.x) got so ingrained that a few other projects started doing the same thing. And it caused lots of problems, and we had to change how the whole release cycle looked. Because it was _incredibly_ painful, and making ready for 2.6.0 was what led me to say that I needed to take a year of unpaid leave from Transmeta exactly because it was getting to be such a big load (that's how I ended up at OSDL, which is now the Linux Foundation - it started out as a way to pay the bills and have health insurance during my year off).
Or all the changes we've done to our source code management. I stayed at patches + tar-balls for the longest time, because it used to work. But I couldn't imagine doing that any more, and we had some painful times with the networking tree using CVS for a while, and trying to sort out the results of that.
So there's been times when it wasn't as much fun, but never really a "walk away" moment, no.
iTWire: What about the BitKeeper episode back in 2005? Was that one of the more stressful moments? And how is your relationship with Andrew Tridgell after that?
LT: So having to drop BK was annoying, but it actually was not nearly as stressful as the events that led up to me _starting_ to use it.
To explain the background, one of the less happy times in kernel development around 2001, with the release of 2.4.0 (and subsequent events). It was one of those major releases, and we'd been doing 2.3.x for a long time, and that was also when the interest in Linux was really taking off in a big way. So things were changing a lot, and the whole development process was really feeling some pain. There were long discussions about patch acceptance (or rather, lack of it - google for "Linus doesn't scale" etc) and we had some serious problems with the VM (largely about that HIGHMEM thing, actually), that were very, very painful.
So that was, I think, the timeframe where the kernel development process _really_ could have failed, and people were unhappy (for various good reasons).
And that timeframe is what explains why I love BitKeeper so much, and have so much respect for Larry McVoy. We ended up solving the VM problems (more or less - there's no way to ever really get the Intel PAE model to work perfectly), but what solved many of the fundamental development problems was getting a source code management system that was really distributed.
That may sound obvious _today_, when people are finally getting pretty used to 'git', and it's widely used even outside the kernel, but in early 2002 when I then started using BitKeeper, not only were there no open source alternatives that were close to viable, almost nobody out there even really _understood_ the point of distributed SCMs. I give Larry McVoy a lot of kudos for pushing BitKeeper, and teaching the world how things should be done. Sure, nothing comes from a vacuum, but still - BK was simply on a different level from anything I had ever seen. To the point that several years later, when I had to drop using it, most everybody _still_ didn't get the point. Due to the licence issue, BK didn't get huge traction even in the kernel community, and so only a fairly small subset of maintainers really "got it" (but enough that during the BK years we had several major subsystems that could be merged through BK, and that made a lot of the scalability issues go away).
So on a stress level, 2001 and 2002 were pretty high. We had serious development flow problems, and me learning the distributed SCM model and just telling people that that's how we're going to do things was certainly not pain-free. But despite all the crap people still give about BK, I'm 100 percent convinced it was absolutely the right thing to do. Because people really don't remember how big a development issue we had before we could have submaintainers that could maintain their own distributed trees and handle distributed merging.
In comparison to that, the couple months of pain in 2005 when BK went away was a walk in the park. It was an annoyance, it wasn't a big fundamental problem with the development model, or a time when I was worried about fundamentals. I even had a ton of fun doing git for a few months.
And yeah, it resulted in awkwardness with Tridge. For a while I really tried very hard to try to find some acceptable common ground where people would be happier with BK, and looking at some way to get Tridge and Larry to come together at some middle ground. For a while I thought that if I could get some nice export format for the BK data, we could get around the major issues. Because at the time there _still_ was no even remotely acceptable open source alternative to BK. And during that time of me looking for solutions, I ended up feeling that "Free Software" (Tridge) vs "Open Source" (me) really hit home at a very practical level.
But as mentioned, I did have fun with git. It resulted in kernel development being shut down for a couple of weeks (and then several months of learning experience and pain for early git adopters), but the end result obviously turned out ok. But I did end up despising most SCM developers out of it all.
iTWire: The number of people and companies that depend on Linux keeps growing year by year. Is this in your subconscious and is there a burden of expectation because of this?
LT: It's not something I really think about all that much, but it does end up impacting the development process. We simply aren't as crazy and care-free as we used to be - it used to be that we could literally just "break the world", and then worry about fixing things up later. These days, we simply can't afford to do that any more. We just need to be more careful, and not as wild and crazy as Linux development used to be.
So it's sometimes less spontaneous, and perhaps less "fun" in a sense. On the other hand, one of the things I've always enjoyed in Linux development has been how it's stayed interesting by evolving. So maybe it's less "fun" in the crazy-go-lucky sense, but on the other hand the much bigger development team and the support brought in by all the companies around Linux has also added its own very real fun. It's a lot more social, for example. So the project may have lost something, but it gained something else to compensate. And that's been true about development in general: development today is still very interesting, even if it's a totally _different_ kind of interesting from what it was almost 20 years ago.
And I wouldn't say it's a "burden". It's resulted in us changing how we do things. These days, exactly to handle the fact that we have all these "serious" users, we have multiple layers of stabilization to try to let people hook into the process at the point that makes sense for them. That starts within the development tree itself (ie the part I maintain, the "upstream kernel"), with the whole notion of a merge window and a separate stabilisation process for each release, but then there's the "stable tree" that is a further stabilisation point, followed by the "long-term releases" which are usually the starting point for "distribution kernels". And then the distros themselves have layers of stabilisation on top of that.
Now, what this all results in is that the actual kernel developers don't need to feel like they have to worry about the staid enterprise users, because there are several layers of quality control and stabilisation between the code jockeys and the people who really don't want to take any risk at all. At the same time, we do try to make sure that the distance between the two points ("developer with new code" and "enterprise user who really doesn't like risk") never gets _too_ wide, because a quick feedback cycle helps everybody. Our fairly short three-month development cycle is geared to that: trying to make it as easy as possible for people to stay as close to the bleeding edge as they are comfortable with.
iTWire: How did this model come about? Is it something that evolved from kernel community discussions or is it something you thought up, suggested and then put into practice?
LT: Pretty much every single development model change has come about as a slow gradual evolution of how we do things, and mostly it has never really been "discussed" or "designed". The shorter development cycle was something that was discussed at one of the kernel summits several years ago, but while it was a big change to how we did things, it was a fairly direct result of everybody realising that we really couldn't continue with multi-year development trees. So it was literally a case of "ok, if two years is too long, how about trying to radically shorten it to two months instead?". And trying it out, and it worked (two months is still the official goal, although everybody knows that in reality it drags out to three months).
So it's not like it was rocket science, it's more a "let's try this and see if it works". The successful models stay around. The "layers of maintenance/stability" is another successful model that we've pretty much always had, it's just that there's more of them. We used to have "Linus does the development tree, Alan Cox does the stable tree, and distributions do their own trees". Now we just have more layers (and the shorter development windows make for more active maintenance trees).
And we have more process in place. We used to have trouble with fixes in the stable tree not always making it to the development tree, so when people started a new stable tree you ended up with old bugs resurfacing that had been fixed once already in the _previous_ stable tree. So now we have the "stable tree patches need to have been in the development tree" rule so that you don't end up in that situation anymore.
But no, there is very little "intelligent design". Most of our process has been small incremental improvements that didn't really need a lot of deep thinking.
iTWire: You seem to be a very low-key person despite all that you've done. How do you manage to keep your feet on the ground?
LT: I've got a huge ego, so I don't know about the whole "low-key" thing.
At the same time, I'm a geek, and not all that social and definitely not interested in the whole public speaking etc side, so I do tend to keep a fairly low profile. And I really enjoy working from home. I also never had any real wish to change the world, or be a visionary - I just wanted to get an OS, and do interesting programming. I never wanted to fly, in other words. I really do enjoy plodding around in the details. Not because I'm humble, but simply because it's what I'm interested in. Maybe that accounts for you thinking I have my feet on the ground.
iTWire: How do you go about prioritising the features for your next release? Is there a lot of lobbying or is it solely an engineering decision?
LT: I'm very seldom feature-oriented. There are some specific cases where I have cared about very specific features (in the current merge window, I made sure that the new filename lookup code got merged, for example), but on the whole what tends to be the most important thing about features are simply whether they are ready, and whether they have actual real users.
A lot of that is tied to our release process - it's largely based on _timing_ rather than on features. If something is ready to be merged, works, and has real users, it gets merged. I will not hold up a release in order for some feature to make it into it, and I don't plan "those features will get into the next release". It's purely a matter of "ok, if was ready for the merge window, and it passed all the criteria, so I'll merge it". And the biggest criterion tends to be that the maintainer of whatever subsystem is happy with it - so it's almost purely an engineering decision.
I say "almost purely", because there is a form of lobbying that works very well. It's simple - companies that have vested interest in certain areas simply pay engineers to get their specific feature done. And obviously engineers are very human, and get attached to and interested in specific issues, and those are often related to things the company they work for is interested in. So I'm not claiming that all decisions are purely based on technical merit, but I think we do have a rather good balance between the pure technical merit, and the wishes of participating companies.
Of course, one thing I've always tried to make sure of is that I'm personally not too tightly associated with any particular Linux player. It's why I've never worked for a Linux distribution, or for any of the big companies making a Linux push. Exactly so that people could trust that at least the top-level maintainer is fairly unbiased and there's no hidden agenda.
iTWire: In your mind, where do kernel features that cater more to the desktop sit?
LT: So to me, personally, the desktop is still where all the interesting stuff is. It's how I personally interact with Linux every day, it's the part of Linux I see most, and it's the area that I think is both the most complex and most influential. It's the one that has the most far-flung problems.
So a kernel feature that helps desktop users is right at the top for me.
That said, there are seldom kernel features that matter hugely (most desktop issues are on the application level), and while the desktop is really important to me, I do have to balance it out with other needs. So if some kernel algorithm doesn't really scale to hundreds of CPUs, we simply can't use it - even if it would work well on the desktop. And I detest "tuning" (because I don't think normal people really ever want to do it, or would know enough even if they wanted to), so it's almost never an issue of "just desktop" vs anything else. Things really do need to work across the board, it's just that desktop testing is one thing I do personally.
But in the last merge window, one of the cool new features we did was this automatic task-grouping for the CPU scheduler, which is pretty much a desktop-only thing. The funny thing was, we actually ended up using code that was designed for specific server setups to implement it. And that's often been the case: features that were _meant_ for one particular niche actually end up being useful for other areas. The whole SMP code, for example, was just for servers not that long ago - very few people had all that many cores on their desktop. These days, of course, if you buy a new machine, you will be hard pressed to find one with just a single CPU in it, even at the low end.
iTWire: What's the one feature you've added or else incorporated into the kernel over the entire development process that's given you the most joy? And what's the one feature that you never liked much but still had to include for reasons beyond your control?
LT: I really can't say that some particular feature has been particularly great or bad, because I've just been doing Linux for too long. We've had lots of great features, and we've had some clunkers.
One thing I still think worked out really well was how we do filename lookup caching (the so-called "dentry" layer). It started out as a patch from a PhD student that did something totally different (it was designed to do file namespace replication over multiple machines) and was really a classic university research project - pretty odd, rather hacky, and not really useful in general. And I just got really excited about the patch, and took the crazy idea and made it do things it was never really meant to do. And these days our whole filename cache is based on that, and the original reason for it is just an obscure detail and was never actually used by anybody afaik.
So that particular thing gave me much joy, because it really worked out so well, and it came from such an odd situation.
When it comes to "feature I had to include for reasons beyond my control", it tends to be about crazy hardware doing stupid things that we just have to work around. Most of the time that's limited to some specific driver or other, and it's not something that has any relevance in the "big picture", or that really affects core kernel design very much. But sometimes it does, and then I really detest it. The whole "support memory past the 4GB mark on a 32-bit architecture" thing is something I absolutely hate. It impacted _everything_. It was really painful, and people trying to run 32-bit x86 processors with 32GB of memory was just a total disaster.
We could do it, but it was a memory management nightmare. And I was really unhappy with how Intel seemed to drag their feet on the whole 64-bit side, pushing instead their crazy and fundamentally flawed Itanium architecture. Then AMD came around with x86-64, and there was finally light at the end of a long, dark, nasty tunnel.
iTWire: From all accounts, you are not only a good manager but also a good family man. How do you manage your time so that nobody feels neglected?
LT: I think it's more that I neglect everybody equally.
I work from home and have very flexible work hours, which makes it a lot easier to be a good dad and help out when help is needed. But at the same time, I do have to say that we have a pretty traditional family model at home, and the real reason my kids don't starve to death is that my wife stays at home and is the one who really takes care of them. I sit in my office and work. A lot. It's not a 9-5 job, it's not even a Mon-Fri job. It's weekends, and it's sitting here answering emails from journalists at 9:30pm.
iTWire: Touche. Do you get lots of requests for interviews? Are there ever times when the Linux Foundation requests you to make a public appearance on their behalf?
LT: Heh. No. I obviously do email interviews, but I don't do a ton of them (it seems to come and go in waves). But it's part of what I consider my job, and I really don't mind it if the questions aren't _too_ inane (which sometimes happens - don't worry, your's haven't been).
And my contract with the Linux Foundation explicitly states that they can't make me do public appearances, or even influence my technical decisions. It's a great contract, there's basically one paragraph saying what I should do (boils down to "maintain the kernel, and everything done on LF time needs to be open source"), while the rest of the contract is about things that I don't have to do.
And the funny part is that I think everybody _likes_ it that way. I obviously do, but even the companies involved in LF and paying dues really all prefer me to be neutral, and not involved in politics. Everybody really is happier knowing that I'm a neutral party, both inside and outside of LF.
Of course, sometimes I still go to LF events. Usually it's somebody saying "Oh, btw, there's world-class diving in xyz, so if you come to the conference you could do some scuba afterwards".
iTWire: Living in America for so many years, you must have remarked on occasion about the differences in attitudes between the people in the US and those in your own native land, Finland. What do you like most about your adopted country? And what do you dislike?
LT: The dislike is the easier one to answer - it's the huge disparity in social and economic circumstances. Coupled to some seriously odd societal hangups ("social safety-nets are bad" or "guns are good, sex is bad" and the occasional "religion is good, science education is bad" crazies).
So one thing I really miss from Finland is the egalitarian society, with strong safety nets, and less of the crazy "punishment" mentality, and more of the "let's help people get away from bad patterns" social model. A couple of years ago, the New York Times ran a big article about the Finnish prison system, and that thing just made me proud to be from Finland. The US really is a third-world country in some respects.
That said, I obviously live here, and not in Finland. Why? Partly simply because the US is bigger. When we moved, we moved because we were young and could try something new, but we moved to the US (and Silicon Valley in particular), because there was an interesting startup doing things that people weren't doing in Finland. I simply wasn't interested in cell-phones, and that's a _big_ deal in Finland. I really do believe that most of the people I went to university with ended up working for Nokia or some related company. A _lot_ of technology in Finland is about mobile phones. Maybe you've played the "Angry Birds" game? That's a Finnish company.
So I moved to the US because I could, and because there was more choice there. And I can talk about "good safety nets" etc all I want, but I'm a programmer, and I make good money. So I'm not personally affected by the lack of safety nets, I just find it disturbing how many people are affected by it, and how everybody still seems to take it for granted. And are often even proud of it.
iTWire: Do you teach your kids about their heritage and encourage them to learn their mother-tongue?
LT: We speak Swedish at home, although with the kids going to regular English-speaking school all their friends obviously speak English, and it has become the stronger language for them, even if Patricia (our eldest) didn't really hear any English at all until she was two or three years old.
They don't even know any Finnish - both me and my wife are of the Swedish-speaking minority in Finland ("finlandssvenskar") and in fact my English is much stronger than my Finnish. I went to Swedish-speaking school all the way to high school, and did 90 percent of my university studies in Swedish too. So to me Finnish was always a distant second language that I'd use mainly when shopping or something like that. My wife went to Finnish-speaking school, but spoke Swedish at home, so while her Finnish is rather stronger than mine, Swedish is her "emotional" language too.
But "teach them about their heritage"? Outside of the language we speak at home, not really.
iTWire: Where do you stand on this whole debate about multiculturalism? Both the US and Australia are melting pots with lots and lots of cultures so the debate is very much alive here. As I guess it must be there too, especially during a time like this when the economy is shot to pieces.
LT: Well, I think one reason I've avoided having to think too much about that discussion is that realistically, it's not like I come from some wildly different cultural background. It's not like the whole "move from Western Europe to the US" was a huge shift of culture. Sure, there are differences, but we're still fundamentally talking "Western culture" and it's not exactly hard fitting in with light skin, blue eyes and brown hair.
And I'm not a huge fan of some rigid multiculturalism and some hardline "you should protect your own culture". If you move to a new place, you certainly don't need to adopt every single cultural notion of that place, but I also think it's crazy and inflexible to think that you should try to take your culture with you. You moved. Deal with it. Don't try to re-create your homeland in the new place. I think it's really sad to see all those "enclaves" where people of the same background move together to make it easier for them to stay the same and keep their old culture. They may make for some great tourist areas or interesting food places, but still...
But as mentioned, it's easy for me to say. I didn't feel (or look) all that out of place.
iTWire: America is a highly religious country. You are, one hears, an agnostic. Does the predominance of religion in everyday life ever prove confronting to you or your family?
LT: There's almost no daily religion that I see, and when I do see any, I'm actually a bit surprised. A large part of that is probably the places we've lived in: neither Silicon Valley nor Portland could even remotely be described as "bible belt", and are both very liberal by US standards - as an European you fit in very well. Yes, there are more churches, and you meet more people who go to church each week (Finland may be 80 percent Lutheran, but people who go to church each week? I don't think I knew many). But religious? Not so you'd really notice it.
Obviously, other parts of the US are different. It _is_ a big country. With a wide disparity in not just social and economic status, but in culture too.
iTWire: So there is no pressure at school for the kids to learn intelligent design and the like?
LT: Creationism/intelligent design? Hell, no. We made sure that our kids go to good schools. The crazies would get laughed out of the place if they tried to bring it up where we live.
iTWire: Have the kids ever asked you questions that would lead to a religious discussion or are they too young for that? If it came up, how would you handle it?
LT: It's come up, but it hasn't been a big deal. The view that all the US is a bible belt really isn't true. Especially not in urban, well-educated areas.
iTWire: Do you ever find yourself reading the source code to other open source operating system (kernels), e.g., NetBSD, to see how a particular feature is implemented? What I mean is, as an author do you have/make time to read code other than kernel code, either for amusement or education?
LT: I haven't ever really found it useful to read other people's code for ideas - source code is a singularly bad medium for transferring high-level concepts, since the whole point is to tell a really stupid computer exactly what to do, rather than explain it at any human level.
So no, I wouldn't read code to see how something is implemented. I'd read code to see why something doesn't _work_ - but then it wouldn't be another OS, it would be something like reading the source code of zlib when I wondered why git spent so much time in some particular library routine ;)
In general, I'd much rather read a book meant for humans than code meant for computers. It's how I started - I still remember Bach's "The Design of the Unix Operating System" fondly. That's how I learnt about how Unix worked. Not to mention Tanenbaum's "Operating Systems: Design and Implementation" book.
These days? No, I don't read OS books any more. I actually seldom read computer books at all.
iTWire: What kind of reading do you do? My special subject for my master's was evolution and I've read that Charles Darwin, after he had worked on The Origin of Species, could not read anything serious in his free time; all that he liked to read was romantic fiction.
LT: Heh, I do read some "serious" books, but they tend to be on biology, human behavior, or physics. But yes, most of the reading I do is really pretty much "crap fiction". It used to be hard science fiction, but these days it's mostly just pretty regular fantasy.
iTWire: What do you do when family or friends ask you to help them fix a problem with their computer, and that computer's not running Linux?
LT: Umm.. Nothing? All the computers in our house are running Linux, so I take care of them, but no, I don't do general computer support.
iTWire: Have you tried to teach any of your children how to program and, if so, in what language?
LT: They haven't shown a lot of interest. If they do, I think I'd start with Python, but who knows? I suspect they'd be better off with just about anybody else teaching them: I don't really have the patience to be a good teacher to begin with, and it's worse when it's your own kids and you get frustrated with them not immediately catching on to some issue. Especially if it's something I consider trivial.
So I occasionally have to go through their math homework with them (because the wife really can't help them), but let's face it - both I and they are happier when I don't need me to.
iTWire: The 2.6 kernel was released way back when. Does the fact that we have yet to see a 2.7 development kernel reflect the fact that there are no new "big ideas" left to explore in the 2-series kernel?
LT: It's more indicative of the whole change of development model. We've been able to introduce pretty big new ideas, but we've done it while not breaking everything, and without having to create a whole new "unstable tree". So we really don't need the pain of having the kind of separate "development kernel" that the 2.1.x, 2.3.x and 2.5.x series were.
We _have_ talked about changing the version numbering, because the "2" at the front seems to be pointless, and the "38" number is starting to be a bit cumbersome (people really aren't very good at remembering the difference between "35" and "36" - it's much easier to keep track of "4" vs "5"). So at the last kernel summit, some people argued for simply dropping the "2" prefix, and turning the "3x" into "3.x", and thus making the numbering a bit more relevant to how we do things. But we did a vote, and there wasn't enough support for it, so nothing happened. But maybe next year more people will have come to the conclusion that the current numbering system is just baroque.
 
#  Linus Torvalds : l’interview anniversaire des 20 ans du noyau)
https://linuxfr.org/nodes/85904/comments/1230981
LinuxFR : You've been doing Linux for about 20 years now and it's a hard job. Is it still fun ?
Linus Torvalds : Oh, absolutely. It's still fun. And partly exactly because I've been doing it for 20 years, I wouldn't call it "hard". It's still challenging and interesting, but I think I'm good at it.
LinuxFR : Why did you choose to switch the kernel from his original non-GPL copyright to the GPL licence ? Was it an ethical or a practical choice ?
Linus Torvalds : Practical. I think my original license contained the ethical parts I cared about, but it turns out that it was too strict about that whole "no money" thing, and it also wasn't well enough known. Moving to the GPL fixed the problems that people had with my original license, and had the advantage that it was a known entity and also a lot more likely to stand up in court than the short blurb I had written originally.
LinuxFR : I know that you consider yourself a very pragmatic person and not a prophet...but do you agree that there is an ethical content in the GPL license ?
Linus Torvalds : I'll answer this two very different ways, and try to explain why I answer it two ways.
So the first and the very negative answer is that I absolutely despise the people who try to push the GPL as being about "ethics".
I think that's absolute bullshit. Why? Because ethics are to me something private. Whenever you use it as an argument for why somebody_else should do something, you're no longer being ethical, you're just being a sanctimonious dick-head.
But the second answer is that I personally feel that the GPL (version 2) matches what I want to do. I really like doing programming, and I wanted to put my stuff out there for others to enjoy, but I felt that the whole "you can do with it as you wish, but your improvements need to be available the same way the original is available" is just very fair, and is a great way to do development.
So personally I think the GPLv2 matches quite closely what I think is "the right way to live my life". And by "right way" I don't think it's the only way. I've done commercial programming too, and I enjoyed that a lot, and I think that was fair and appropriate too (hey, they paid me for it).
So I think the GPLv2 is a great license, and I use it for my own personal reasons. I do think that's true of a lot of other people too, but I really want to point out that it's not that the license is somehow ethical per se. A lot of other people think that the BSD license with its even more freedoms is a better license for them. And others will prefer to use a license that leaves all the rights with the original copyright holder, and gives no rights to the sources at all to others. And for them, that is their answer. And it's fine. It's their choice.
Trying to push any particular license as "the ethical choice" just makes me mad. Really.
LinuxFR : Why is the desktop so special and so much harder than any other market ?
Linus Torvalds : Because it's so much more interesting. It's the market where people do so many different things.
Your average server does almost nothing. Sure, it may have a lot of CPU power, a fast network, and lots of IO, but it does the same thing over and over again, and that "same thing" is pretty limited. It's running a database, a mail or web server, various analytics etc. It may be important for the company, but it's not a very varied workload,
and it's not something people are attached to.
In contrast, your desktop is what you see every day, and you get attached to it. The attachment might be some kind of "stockholm syndrome" where you really don't necessarily like it (think of all the people who grew to know computers through DOS and Windows, and the three-finger ctrl-alt-del salute), but even then it becomes a kind of dependency where you get used to it and rely on it rather more intimately than you ever end up relying on the company mainframe.
And the desktop does so much more. You play games on it, you do word processing on it, you do development on it. It's not a single-trick pony. Of course, for some people it has also become "you use a web browser on it" and nothing much more, but even that single use tends to in many ways be a more complex workload than most server workloads.
Of course, what's interesting is how smartphones are slowly starting to share many of those desktop complexities. It may not be there yet (and maybe it never will be), but phones already have a fair amount of the same rich and complex media issues that desktops have, and are getting more varied uses.
LinuxFR : Why Linux desktop hasn’t been adopted by the mainstream users ? Is it possible for the kernel community to improve this situation or is it mostly a user space problem ?
Linus Torvalds : I don't think there's much we can do on the kernel side, except continue to improve in general, and make sure we remain the technically best choice.
And it's not really that we don't have mainstream users - Android is an example of lots of mainstream users for Linux. The problem is that the desktop is a difficult market to reach. There are huge amounts of "network effects" where having existing users is a big reason to retain them and get more new users. It's also that most people really
really don't want to change their environment, and if they do switch, they want help and support. Where "support" is not necessarily about commercial support, it's about knowing that you have people around you who know the system and can give you advice etc.
Getting past that hump is hard. And it's not something that happens by pointing at technical issues. It's often a social issue.
LinuxFR : I know it's a strange (and probably dumb) question but are you still able to fully understand all parts of Linux kernel or do you really need trusted maintainers ? For example concerning the complex pathname lookup patchs from Nick Piggin what was your process to choose between these patchs and the other solution from Dave Chinner ? Have you received some advices from Al Viro or was it your lone decision ?
Linus Torvalds : Oh, I absolutely don't claim to know and understand all of the kernel. I know a wider chunk of it than most kernel developers, but there are many areas where I almost entirely rely on the maintainers, because I just don't know (or necessarily even care - we all have our own areas of interests) enough about some subsystem.
That said, the example you picked wasn't one of those areas. The VFS layer and most of the VM layer I'm intimately familiar with, and so I had no problems feeling like I could make those decisions on my own, and support the end result even without further help. Of course, that's not to say that I didn't expect people like Al to help me, and
I didn't talk it over with them, but when it came to the new pathname lookup, I was much more personally involved than I am in most other areas.
Of course, usually I don't really want to feel like I need to be personally involved. The pathname patches had been around for most of a year, and weren't making much progress (there was a lot of discussion, but not as much forward progress as I wanted), so I ended up championing those patches a bit more than I would necessarily
otherwise have done. I'd have been perfectly happy having them go entirely through the normal submaintainer channels.
In some other cases, I can't do that kind of "I'll step in and make some executive decision", because I don't necessarily know the area well enough, or I don't feel like I can really end up maintaining the end result if I really piss off the maintainer too badly. So then I can prod and try to raise discussion about issues, but I'll just have
to trust that somebody gets up and makes the right decision.
(Btw, "right decision" is not necessarily the right word. Sometimes you just need to make a decision. It's not always clear what the "right" answer is, and sometimes it's fine to just say "we don't know" and not make a decision at all. But at other times you really do need to make some kind of technical choice. The good news is that most
technical choices can be reversed if they turn out to be wrong later on - it may be very painful, but sometimes it's better to just make a choice even if you can't know whether it's the right one. Even if there is a chance that you may have to reverse that choice later)
I should say that those kinds of situations are actually pretty rare. Most of the time the development process works without anybody having to make particularly difficult choices. Most of the time it's fairly clear that the forward direction is, and sometimes it's just hard to find people that actually write the code :)
LinuxFR : What do think about this citation from Lennart Poettering ? : "In fact, the way I see things the Linux API has been taking the role of the POSIX API and Linux is the focal point of all Free Software development. Due to that I can only recommend developers to try to hack with only Linux in mind and experience the freedom and the opportunities this offers you. So, get yourself a copy of The Linux Programming Interface, ignore everything it says about POSIX compatibility and hack away your amazing Linux software. It's quite relieving!"
Linus Torvalds : Well, I think it can be a reasonable way to simplify a very difficult problem (portability).
But one reason it's likely to be a good way is that we really don't try to extend too much - and not unnecessarily - on the standard interfaces, so even if you do end up deciding to just care about Linux, it's really hard to make the end result horribly unportable. Linux is a pretty middle-of-the-way UNIX in many respects, and even
outside of the unixes, most of the frameworks that you'd use on Linux are available on other platforms too (often because they are open source).
So even if you later decide that you do want to be portable after all, going with Linux to begin with was probably not a bad idea, and it may have simplified the early decision process in a project.
LinuxFR : Do you think systemd is a huge improvement in comparison to SysV init ? Is it a game changing technology ?
Linus Torvalds : I also will take a somewhat wait-and-see approach, it's not widely enough used yet. I do think bootup performance is important, and anything that helps that, and helps make it more flexible is a good thing. Would I call it "game changing"? Probably not.
LinuxFR : Do you use btrfs ? When do you think it will be ready to replace ext4 as the default recommended file system ?
Linus Torvalds : I've used btrfs on a couple of the laptops I have, but honestly, when it comes to filesystem usage, the big factor is distribution support. Almost nothing else matters, as long as the core capability is there. So getting a distribution to pick a filesystem by default is the way it ends up being the commonly recommended one, simply because what most people care about in filesystems is neither features nor performance, but stability.
And me personally, I don't worry about it any more. I had some issues with ext3 (horrible fsync performance in particular), but most of what I do is actually totally dominated by the VFS layer, because it caches so well (ie I have the kernel sources in memory pretty much all the time, and the filesystem doesn't matter because all the caching is
done by generic routines).
LinuxFR : With Linux and Git you've done two softwares that changed the computer world. How is it possible to have two gigantic success like these ? What is the difference between you and mere mortals ?
Linus Torvalds : I think a lot of it is just stubborn persistence. Especially Linux - I was in the right place at the right time, but others could have done their own operating system. And others did. But very few people ended up just doing it as much as I did. I've been at it for twenty years. Most developers doing some random private project for their own enjoyment don't stay with it long enough for it to ever be even release ready. Much less for decades.
With git, things were a bit different. Partly because I had a use case that was big enough (the kernel) that I could jump-start it, but partly because I really came at an old problem from a new angle, and I do think I did a good job as "architect" for the project. I really think that git has a fundamentally good design (in the case of Linux I
could just build on top of the design of Unix that had already been done), and I could push it through because Linux needed more than the existing SCM projects could offer.
But with git, a lot of the credit really does go to Junio Hamano. He's really been a great maintainer. People like that are important. Yes, open source programming is a team sport, but finding the right people is really the killer feature (the same is obviously true in the kernel too - I really do think we have a great set of maintainers. I may
complain about them and I'm somewhat infamous for my flames when things don't work, but at the same time I'm convinced there's some of the best people out there working on maintaining the kernel).
LinuxFR : I've seen that you read a lot about biology and evolution theory (for example Richard Dawkins). Is that knowledge useful for Linux process developpement ? Is it mostly a darwinian competitive model or a cooperative one ?
Linus Torvalds : I don't think it's useful for the kernel, but it does happen to be a subject I'm interested in. Biology, evolution, human behavior - they are all fascinating subjects. And I think there are parallels between technological development and evolution. I obviously do not believe in Intelligent Design when it comes to biology (and I think anybody who does is woefully badly educated), but after all, I often think that "intelligent design" is much overrated in technology too. Many technical improvements really do seem to be about more "organic" developments, and very few are fully designed ahead of time. In fact, I think most interesting technical problems are too complicated to "design" for - the way you get to a solution is very much through incremental improvements and trial and error.
LinuxFR : You are now an american citizen. What do you think about software patent law in USA ? Is your voice strong enough to help fight this law in this country ?
Linus Torvalds : I have to admit that while I don't like patents, I also tend to try to stay away from getting too involved in issues like those. I'm good at what I do, I think there are people who are better at fighting the patent mess. And I think you need to do so from within the system - so I'd expect that any solution will actually largely come from all the companies who are being hurt by the mess that it is.
LinuxFR : You often wrote detailled posts on realworldtech.com but I've never seen a post from you on lwn.net. Why ? Do you read lwn.net regularly ?
Linus Torvalds : Heh. rwt is where I go to flame people and argue about computer architecture. I like people who argue back, and I also like how it's not Linux people, or even about Linux. So I can spout my opinions and see what arguments I can get to happen.
lwn? That would be an entirely different kettle of fish.
LinuxFR : Various people criticized kernel security. Do you think kernel devs put enough work on kernel security mechanisms and code review ? Do you think some parts of GRSecurity should be imported in mainline ?
Linus Torvalds : I think we've done a pretty good job, but one of the problems with the security world is that it's so black-and-white. To some security people, even me just saying "pretty good job" is like a red flag. They don't think "pretty good" is good enough, they think security is somehow everything.
And I disagree. Most security problems are just bugs. We need to try to avoid bugs, but bugs happen. That's a truism, but I think it's what it all boils down to. Are we better at avoiding bugs than other projects? I think we're doing pretty well, especially considering just the insane amount of code we write and change every single day. I really don't think it's useful to try to separate "security bugs" from other bugs. They're all just bugs, and almost any bug could be a security bug under the right circumstances.
As to grsecurity, I think we've taken the important things, and left the extreme things (the ones that actually hinder usability or are otherwise extremely intrusive) behind.
In general, the best way to security tends to be to have many different layers of security. You can never be entirely bug-free in any half-way interesting program, but with multiple layers of security, one layer will hopefully end up protecting against the possibility of a bug at another layer that might otherwise have resulted in an exploit.
So in the kernel, we tend to try to have the generic layers already have security checks early, so even if there's a bug in a filesystem or a driver, it's harder to cause that bug to be exploitable. And when we do find some overflow, we tend to fix both the immediate overflow itself as well as (when possible) add checks at higher levels so that the overflow couldn't have happened in the first place. So in many cases we actually end up with several fixes to the same problem - each of which would have fixed things on its own, but together they end up hopefully being more resistant to a similar problem happening somewhere else (or later be re-introduced in the same place - it's embarrassing when it happens, but that has happened too).
LinuxFR : Do you think security experts and exploits creator have a different mindset compared to other kernel developpers ?
Linus Torvalds : Oh yes. Some of the more interesting exploits in particular have made me go "that needs a really twisted mind to come up with" and being quite impressed.
But it's not always about being impressed. Quite often I get rather depressed by all the sleaze in security circles. It really is a circus. A lot of it is all about posturing and PR (on all sides:vendors, security people, exploiters etc).
LinuxFR : I've done a interview with Brad Spengler (GRSecurity) and asked a question about his opinion on you and kernel security. Brad's answer was: "He at times can understand security better than some of the other developers, but security isn't one of his main goals (...) His ideas regarding security that have formed the official policy of kernel developers to omit security-relevant information in changelogs and treat all bugs the same are destructive".
Do you think all bugs should be treated the same ? And why ?
Linus Torvalds : I tend to think that all bugs should be treated the same, and I neither want to point out, nor particularly hide our security issues.
The thing is, even security people cannot agree. Many of them want full disclosure. Others want very limited disclosure (vendors and big financial institutions). Others want just total secrecy, both to avoid embarrassment, but also at least to some degree to avoid the issue leaking to the "black hat" people who write exploits.
And then there's the fights about leaks - everybody knows that there are different classes of "bad guys", ranging from just regular good people who want to try things out (hey, university kids who hear about a new exploit, and decide that they want to test whether the big university machines really do crash) to script kiddies who like being annoying but don't necessarily have all that impressive technical background and understanding, to people who are really smart and may well be doing things for serious criminal gain.
How the hell do you resolve those discussions? I claim that you can't. There is no "right way", and the people who push their way of doing disclosure (or not) are demonstrably doing bad things.
So my personal opinion is that the only sane approach is to just realize that it's not a solvable issue, and just treat bugs as bugs. We try to avoid having them in the first place, but when they do happen, we fix them. And we fix them without shouting from the rooftops about the details about how to exploit the issue, and without even trying to make it easy for the people who might want to try to exploit things to find them. And yes, that can very much involve not saying everything we know about how to exploit the bug in the changelog, or even necessarily pointing out that there is an advisory about it.
Do security people always agree with me? Hell no. But they don't agree amongst themselves either, so what does that prove?
LinuxFR : Do you have an opinion on OpenBSD quality ? They put strong emphasis on security. Is there some lessons to learn from this project ?
Linus Torvalds : I think that any single-purpose OS project is a failure, and it doesn't matter what the purpose is. Security on its own is not a worthy goal - you need to have users for security to even matter in the first place.
So I think the single-mindedness about security in OpenBSD just makes the whole project less interesting and relevant in the end.
But again, that's just my general "bugs are bugs" thinking. I think security is important, but so is performance, and so are features. The world just isn't black-and-white. There is no one single thing that matters more than other things.
LinuxFR : LLVM compiler is making huge progress. What do you think about this project ? Is LLVM architecture better than GCC and do you think it will replace GCC in the long run ?
Linus Torvalds : Replace? Might happen, but I don't think it's particularly likely. I do find compilers interesting, and I think it's good to have some competition in that area, and so I like seeing LLVM make the effort.
LinuxFR : There is a Linux kernel on my ADSL box sent by my Internet service provider. There is also a Linux kernel on my Sony TV and on my printer. However I'm not free to hack on my ADSL box, my TV or my printer (due to legal reasons or due to tivoization). What do you think about this situation ?
Linus Torvalds : I personally think flexible hardware is more interesting than locked down hardware, but at the same time, to me the "fair exchange" was always about the software and the ideas, not about hardware.
So my whole issue with Tivo and company has always been "hey, they designed and built the hardware, the fact that they used my software doesn't give me any special rights to the hardware".
Since they are using Linux, I do expect them to make their changes to Linux source code available to people as the license requires. There obviously have been cases of companies not doing even that, but that's the exception rather than the rule.
So you can take that Linux source code with their modifications, and design and build your own ADSL box or TV or whatever and use whatever improvements to Linux that they did. Or more relevantly, you can use their Linux improvements EVEN IF YOU DON'T make an ADSL box - you can do it on your desktop, or on some totally unrelated computer, and that's when the improvements become rally interesting - when you use them for something else than they were originally meant for.
Now, of course, most Linux users in that space don't actually need to make all that many changes to the kernel, so there may not be any improvements. That's fair too. If Linux worked for them without modifications, then that's fine. Again, it will work for you without modifications if you want to do some similar hardware.
So I always thought the whole "Tivoization" thing was silly thing. If you want to make your own Tivo, just do it. Don't think that just because it runs open source you should control the hardware. It's open source. If you want to make open hardware, make open hardware.
Now, that said, I do think that there are serious problems in the content industry, where content providers are using laws and technical measures to basically try to lock people in and create more of a monopoly situation. I don't like DRM. But I think that's a different issue from the software license, and I also think that it was seriously wrong of the FSF to try to use the GPLv3 as a way to make other peoples software projects into weapons in their fight against DRM. And I'm very happy that I had made it clear that Linux was a GPLv2-only project many years before that all happened.
LinuxFR : What is your opinion about Android ? Are you mostly happy they made cellphones very usable or sad because it's really a kernel fork ?
Linus Torvalds : I think forks are good things, they don't make me sad. A lot of Linux development has come out of forks, and it's the only way to keep developers honest - the threat that somebody else can do a better job and satisfy the market better by being different. The whole point of open source to me is really the very real ability to fork (but also the ability for all sides to then merge the forked content back, if it turns out that the fork was doing the right things!)
So I think the android fork forced the mainline developers to seriously look at some of the issues that android had. I think we've solved them in mainline, and I hope (and do think) that android will eventually end up merging to mainline. But it will probably take time and further effort.
I think the more serious long-term issue we have in the kernel is the wild and crazy embedded platform code (and mostly ARM - not because ARM is in any way fundamentally crazier, but because ARM is clearly the most successful embedded platform by far). The embedded world has always tended to eschew standardized platforms: they've been resource constrained etc, so they've done very tailored chip and board solutions, and felt that they couldn't afford a lot of platform abstraction.
That causes a big maintenance headache, because then all those crazy platforms look slightly different to the kernel, and we have all that silly code just to support all those variations of what is really just the same thing deep down, just differently hooked up and with often arbitrary small differences.
But that's something that happens both within and outside of Android, it's in no way android-specific.
LinuxFR : What about the technical differences between Android and mainline ? Do you think the "wakelock" controversy is solvable ?
Linus Torvalds : I think it is technically largely solved (ie "details to be fixed, but nothing fundamentally scary"), but practically once you have an interface and existing code, it just is a fair amount of work to change. And there perhaps isn't quite enough motivation to make those changes very quickly. So it will take time, and probably several releases (both mainline and adroid) to actually happen.
LinuxFR : Windows 8 will run on ARM. Is this a real threat to Linux dominance on embedded market?
Linus Torvalds : It's not how I think, or care. Linux competes with itself, not with Windows. IOW, at least as far as I'm concerned, the thing I want to make sure is that Linux just improves.
If anything, I suspect that in order to support ARM, MS will end up forcing some platform standardization on the ARM setups they support, and will make our job easier. I won't mind that at all.
LinuxFR : Can you explain why you're not happy with the ARM patches sent to you during merge windows ? Is there an obvious solution for this fragmentation problem ?
Linus Torvalds : Obvious solution? No. The problem is the wild variety of hardware, and then in many cases the Linux ARM platform code (not the ARM CPU support, but the support for certain chips with all the glue issues around the CPU core) has been mostly ugly "copy-and-paste" from some previous ARM platform support file, with some minimal editing to make it match the new one.
And it just results in this unmaintainable mess. It becomes painful when somebody then fixes some core infrastructure, and you end up with a hundred different ARM files all using that infrastructure. That happened with the IRQ chip driver cleanups Thomas did recently (well, has been doing over along time, the recent part is really just the final removal of some nasty old interfaces).
It results in other maintainability issues too - patches being big just means that people won't look at them as carefully etc etc. So it's just a bad situation. Many of the cases should be solvable by having better generic solutions and then plugging in just some per-platform numbers for those solutions.
LinuxFR : What is your opinion about microkernels ? Do you still think it's a technical failure ?
Linus Torvalds : Yes, I'm still convinced that it's one of those ideas that sounds nice on paper, but ends up being a failure in practice, because in real life the real complexity is in the interactions, not in the individual modules.
And microkernels strive to make the modules more independent, making the interactions more indirect and complicated. The separation essentially ends up also cutting a lot of obvious and direct communication channels.
LinuxFR : What about managed OS like Singularity ? Is it research only or do you think it can work ?
Linus Torvalds : I'm a rather harsh and pragmatic person. Right now it looks like research only. The devil really is in the details, and a lot of these nice theoretical frameworks talk about the big issues, but not about all the details that you hit when you have all those crazy users that do odd (and wonderful) things.
And the thing is, I really don't see the point. Operating systems aren't that complicated. There's nothing wrong with the UNIX model. Sure, we have a lot of code, and I'm not claiming it's simple code, but it's quite manageable. The biggest problems we tend to have in Linux is driver and platform support - not things that are fixed with some new programming model. Microkernels or managed code? They really solve none of the big problems I see.
LinuxFR : Imagine we're in 2031 and the Linux kernel is now forty years old. Are you still the project leader ? Do you think the kernel is more or less the same than in 2011 or do you think there are new radical innovations included ?
/troll mode: Was it rewritten in C++ ?
Linus Torvalds : I really really hope that by 2031, we'll have moved past the OS as a place for radical ideas. Sure, it will run some OS, and I hope it will be Linux, but all the radical and interesting work had better be done in user space and in giving us more exciting interfaces to the computing power.
So I personally do think that the kernel approach will still be valid and that the changes will be at a different level.
After all, we've had forty years of Unix, and that whole "monolithic kernel in C" hasn't become invalid in those forty years. Sure, the details have changed, the language has evolved, and we have way more complex interfaces, but the basic design is still quite recognizable. And I don't think another 20 years will necessarily change that at all.
LinuxFR : Thanks a lot for your answers...and happy birthday for the kernel :-)
# A conversation with Linus at LinuxCon Japan
https://lwn.net/Articles/445687/
