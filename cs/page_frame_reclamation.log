#---
tags: [ kernel ] 
layout: post
date: 2014-12-28
title:  Page frame reclaimation
category: cs
---

# References
[Chapter 10 Page Frame Reclamation](https://www.kernel.org/doc/gorman/html/understand/understand013.html)
[Low_On_Memory](https://linux-mm.org/Low_On_Memory)
[Add predictive memory reclamation and compaction](https://lkml.org/lkml/2019/8/12/1302)

# Page flags
PG_reclaim             18      /* To be recalimed asap */ Also see end_page_writeback().
tglx: commit 3b0db538ef6782a1e2a549c68f1605ca8d35dd7e
Author: Andrew Morton <akpm@digeo.com>
Date:   Mon Dec 2 21:31:09 2002 -0800
    [PATCH] Move reclaimable pages to the tail ofthe inactive list on
PG_writeback
tglx commit d58e41eec6859e9590f8f70ccdc1d58f4f6a1b84
Author: Andrew Morton <akpm@zip.com.au>
Date:   Sun May 5 01:10:37 2002 -0700
    [PATCH] Fix concurrent writepage and readpage

# Scan control: may_unmap vs may_swap
a6dc60f8975a ("vmscan: rename sc.may_swap to may_unmap")
2e2e42598908 ("vmscan,memcg: reintroduce sc->may_swap") reintroduced .may_swap for memory controller.

# Kswapd background reclaim
kswapd->balance_pgdat; pgdat->kswapd_wait __GFP_KSWAPD_RECLAIM wake_all_kswapds()
Check kswapd_try_to_sleep: premature sleep and full sleep.[Historically, kswapd used to wake up every 10 seconds but now it is only woken by the physical page allocator when the pages_low number of free pages in a zone is reached](https://www.kernel.org/doc/gorman/html/understand/understand013.html#toc71)

# Direct reclaim 
__alloc_pages_direct_reclaim->__perform_reclaim->try_to_free_pages: throttle_direct_reclaim, do_try_to_free_pages


# Remove from page cache or swap cache
## !mapping for truncate (see call sites of __delete_from_page_cache)
Firo: page->mapping set to NULL, but left on the LRU list due to page->_count > 2;
commit 3aa1dc772547672e6ff453117d169c47a5a7cbc5
Author:     Andrew Morton <akpm@zip.com.au>
AuthorDate: Wed Aug 14 21:20:48 2002 -0700
    [PATCH] multithread page reclaim
[...]
+                * Rarely, pages can have buffers and no ->mapping.  These are
+                * the pages which were not successfully invalidated in		# 'not successfully' means that put_page() fails.
+                * truncate_complete_page().  We try to drop those buffers here
+                * and if that worked, and the page is no longer mapped into
+                * process address space (page_count == 0) it can be freed.
+                * Otherwise, leave the page on the LRU so it is swappable.

commit 28e4d965e6131ace1e813e93aebca89ac6b82dc1
Refs: v2.6.18-1583-g28e4d965e613
Author:     Nick Piggin <npiggin@suse.de>
AuthorDate: Mon Sep 25 23:31:23 2006 -0700
    [PATCH] mm: remove_mapping() safeness
    Some users of remove_mapping had been unsafe.
    Modify the remove_mapping precondition to ensure the caller has locked the
    page and obtained the correct mapping.  Modify callers to ensure the
    mapping is the correct one.
## remove_mapping introduced from initial version of page mirgation
commit 49d2e9cc4544369635cd6f4ef6d5bb0f757079a7
Refs: v2.6.15-1461-g49d2e9cc4544
Author:     Christoph Lameter <clameter@sgi.com>
AuthorDate: Sun Jan 8 01:00:48 2006 -0800
    [PATCH] Swap Migration V5: migrate_pages() function

## Legacy memcg dirty page throttling
commit e62e384e9da8d9a0c599795464a7e76fd490931c
Refs: u3.5-8304-ge62e384e9da8
Author:     Michal Hocko <mhocko@suse.cz>
AuthorDate: Tue Jul 31 16:45:55 2012 -0700
    memcg: prevent OOM with too many dirty pages
    The current implementation of dirty pages throttling is not memcg aware
    which makes it easy to have memcg LRUs full of dirty pages.  Without
    throttling, these LRUs can be scanned faster than the rate of writeback,
    leading to memcg OOM conditions when the hard limit is small.
    This patch fixes the problem by throttling the allocating process
    (possibly a writer) during the hard limit reclaim by waiting on
    PageReclaim pages.  We are waiting only for PageReclaim pages because
    those are the pages that made one full round over LRU and that means that
    the writeback is much slower than scanning.
[...]
+                        * memcg doesn't have any dirty pages throttling so we
+                        * could easily OOM just because too many pages are in
+                        * writeback from reclaim and there is nothing else to
+                        * reclaim.
+                        * Check may_enter_fs, certainly because a loop driver
+                        * thread might enter reclaim, and deadlock if it waits
+                        * on a page for which it is needed to do the write
+                        * (loop masks off __GFP_IO|__GFP_FS for this reason);
+                        * but more thought would probably show more reasons.
+                       if (!global_reclaim(sc) && PageReclaim(page) &&
+                                       may_enter_fs)
+                               wait_on_page_writeback(page);
+                       else {
+                               nr_writeback++;
+                               unlock_page(page);
+                               goto keep;
[...]


## legacy memcg writeback page which should be set to PG_reclaim
commit c3b94f44fcb0725471ecebb701c077a0ed67bd07
Refs: u3.5-8305-gc3b94f44fcb0
Author:     Hugh Dickins <hughd@google.com>
AuthorDate: Tue Jul 31 16:45:59 2012 -0700
    memcg: further prevent OOM with too many dirty pages
[...]
    This residual problem comes from an accumulation of pages under ordinary
    writeback, not marked PageReclaim, so rightly not causing the memcg check
    to wait on their writeback: these too can prevent shrink_page_list() from
    freeing any pages, so many times that memcg reclaim fails and OOMs.
    Deal with these in the same way as direct reclaim now deals with dirty FS
    pages: mark them PageReclaim.  It is appropriate to rotate these to tail
    of list when writepage completes, but more importantly, the PageReclaim
    flag makes memcg reclaim wait on them if encountered again.  Increment
    NR_VMSCAN_IMMEDIATE?  That's arguable: I chose not.
    Setting PageReclaim here may occasionally race with end_page_writeback()
    clearing it: lru_deactivate_fn() already faced the same race, and
    correctly concluded that the window is small and the issue non-critical.
[...]
    Trivia: invert conditions for a clearer block without an else, and goto		# Firo: unclearer
    keep_locked to do the unlock_page.
# pagevec and rotate
commit 902aaed0d983dfd459fcb2b678608d4584782200
Refs: v2.6.23-4285-g902aaed0d983
Author:     Hisashi Hifumi <hifumi.hisashi@oss.ntt.co.jp>
AuthorDate: Tue Oct 16 01:24:52 2007 -0700
    mm: use pagevec to rotate reclaimable page
See lru_rotate_pvecs; Firo: seems all pages on lru_rotate_pvecs are get_page(). 
Also see rotate_reclaimable_page().
