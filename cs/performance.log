
# Concepts
Disk I/O latency: = response time = Service time + Wait time
Network latency: can refer to the time it takes for a message to make a round trip between endpoints.
I/O wait: account_idle_time

# Instrumentation
tracepoints

# Backlog
http://oliveryang.net/2017/12/linux-high-loadavg-analysis-1/

Something like this:

  taskset 1 perf stat -a -e '{instructions,cycles}' --repeat 10 perf bench sched pipe

... will give a very good idea about the general impact of these changes on 
context switch overhead.
https://lore.kernel.org/lkml/1471106302-10159-5-git-send-email-brgerst@gmail.com/T/#u

[Off-CPU Analysis](http://www.brendangregg.com/offcpuanalysis.html#Analysis)

# Tools
[Give me 15 minutes and I'll change your view of Linux tracing](https://www.youtube.com/watch?v=GsMs3n8CB6g)

# PELT - load-avg
commit 5b51f2f80b3b906ce59bd4dce6eca3c7f34cb1b9
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:32 2012 +0200

    sched: Make __update_entity_runnable_avg() fast

commit a481db34b9beb7a9647c23f2320dd38a2b1d681f
Refs: v4.11-rc2-229-ga481db34b9be
Author:     Yuyang Du <yuyang.du@intel.com>
AuthorDate: Mon Feb 13 05:44:23 2017 +0800
Commit:     Ingo Molnar <mingo@kernel.org>
CommitDate: Thu Mar 30 09:43:41 2017 +0200
    sched/fair: Optimize ___update_sched_avg()
+       /*
+        * Now we know we crossed measurement unit boundaries. The *_avg
+        * accrues by two steps:
+        *
+        * Step 1: accumulate *_sum since last_update_time. If we haven't
+        * crossed period boundaries, finish.
+        */
+       if (!accumulate_sum(delta, cpu, sa, weight, running, cfs_rq))
+               return 0;

-       if (decayed) {
-               sa->load_avg = div_u64(sa->load_sum, LOAD_AVG_MAX);
-               if (cfs_rq) {
-                       cfs_rq->runnable_load_avg =
-                               div_u64(cfs_rq->runnable_load_sum, LOAD_AVG_MAX);
-               }
-               sa->util_avg = sa->util_sum / LOAD_AVG_MAX;
+       /*
+        * Step 2: update *_avg.
+        */
+       sa->load_avg = div_u64(sa->load_sum, LOAD_AVG_MAX);
+       if (cfs_rq) {
+               cfs_rq->runnable_load_avg =
+                       div_u64(cfs_rq->runnable_load_sum, LOAD_AVG_MAX);
        }
+       sa->util_avg = sa->util_sum / LOAD_AVG_MAX;

