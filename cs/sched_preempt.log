# Problems
[Optimizing preemption](https://lwn.net/Articles/563185/)
https://www.kernel.org/doc/Documentation/preempt-locking.txt

## Preempt disable vs barrier()
commit 386afc91144b36b42117b0092893f15bc8798a80
Refs: v3.9-rc6-22-g386afc91144b
Author:     Linus Torvalds <torvalds@linux-foundation.org>
AuthorDate: Tue Apr 9 10:48:33 2013 -0700
    spinlocks and preemption points need to be at least compiler barriers

## [Voluntary Kernel Preemption, 2.6.12-rc4-mm2](https://lwn.net/Articles/137259/)
Voluntary preemption works by adding a cond_resched() 
(reschedule-if-needed) call to every might_sleep() check. It is lighter 
than CONFIG_PREEMPT - at the cost of not having as tight latencies. It 
represents a different latency/complexity/overhead tradeoff.

[voluntary preemption](https://stackoverflow.com/questions/5174955/what-is-voluntary-preemption)
commit 41719b03091911028116155deddc5eedf8c45e37
Refs: v2.6.29-rc1-226-g41719b030919
Author:     Peter Zijlstra <a.p.zijlstra@chello.nl>
AuthorDate: Wed Jan 14 15:36:26 2009 +0100
Commit:     Ingo Molnar <mingo@elte.hu>
CommitDate: Wed Jan 14 18:09:00 2009 +0100
    mutex: preemption fixes
    The problem is that dropping the spinlock right before schedule is a voluntary
    preemption point and can cause a schedule, right after which we schedule again.
    Fix this inefficiency by keeping preemption disabled until we schedule, do this
    by explicity disabling preemption and providing a schedule() variant that
    assumes preemption is already disabled.
Firo: spin_unlock_mutex
## User preemption - Linux kernel user mode is always User preemption.
system call returns mode . syscall_return_slowpath
interrupt hander returns user mode .retint_user->prepare_exit_to_usermode
## Linux kernel kernel mode is coppertive when CONFIG_PREEMPT is not set.
bloked (which results in a call to schedule())
If a task in the kernel explicitly calls schedule() it's involuntary!!!
## Linux kernel kernel mode is coppertive + preemptive when CONFIG_PREEMPT is set.
* When an interrupt handler exits, before returning to kernel-space.
retint_kernel->preempt_schedule_irq->cond_resched
* __local_bh_enable_ip -> preempt_check_resched
## The following also t relates to preemption; it's PREEMPT_VOLUNTARY.
For example, in might_resched(). The task willingly yeilds the CPU, but it should stay on rq. 
config PREEMPT_VOLUNTARY
        bool "Voluntary Kernel Preemption (Desktop)"
        help
          This option reduces the latency of the kernel by adding more
          "explicit preemption points" to the kernel code. These new 
          preemption points have been selected to reduce the maximum
          latency of rescheduling, providing faster application reactions,
          at the cost of slightly lower throughput.
* need_resched - When kernel code becomes preemptible again.
1. set_tsk_need_resched() in resched_curr
tick: check_preempt_tick or entity_tick
fork: wake_up_new_task->check_preempt_curr->check_preempt_wakeup
wakeup: check_preempt_wakeup
...
2. if (need_resched()) cond_resched();

# Translation
## CONFIG_PREEMPTION
commit b8d3349803ba34afda429e87a837fd95a99b2349
Refs: v5.3-rc1-1-gb8d3349803ba
Author:     Thomas Gleixner <tglx@linutronix.de>
AuthorDate: Mon Jul 22 17:59:19 2019 +0200
Commit:     Thomas Gleixner <tglx@linutronix.de>
CommitDate: Mon Jul 22 18:05:11 2019 +0200
    sched/rt, Kconfig: Unbreak def/oldconfig with CONFIG_PREEMPT=y

## LQO
* if (!preempt && prev->state)in __schedule; why prev->state?
prev->state means deactivate.
