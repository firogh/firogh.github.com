# E820 memory map
Kerenl setup header?: detect_memory() Load BIOS memory map into boot_params.e820_map
setup_arch->
        setup_memory_map -> default_machine_specific_memory_setup // Save e820 memory map into struct vriable e820 from boot_params.e820_map.
        max_pfn = e820_end_of_ram_pfn(); // max_pfn  BIOS-e820: mem 0x0000000100000000-0x00000003227fffff usable 
					// and last_pfn = 0x322800(12840MB), so last_pfn is invalid address, use it with <. 
        mtrr update max_pfn, see [Processor supplementary capability](https://en.wikipedia.org/wiki/Processor_supplementary_capability)
        trim_low_memory_range // reserve 64k 
        max_low_pfn = e820_end_of_low_ram_pfn(); //end of block below 4GB 

# KSM
https://bugzilla.suse.com/show_bug.cgi?id=1119962#c16
[mm: put_and_wait_on_page_locked() while page is migrated](https://marc.info/?l=linux-mm&m=154711241119699&w=2#1)
[Bug 1144338 - L3: ksm_max_page_sharing feature in the SLES kernel, was BMW is experiencing peaks of VM stalls ranging from 15 to 30 minutes](https://bugzilla.suse.com/show_bug.cgi?id=1144338)

# Hotplug
add_memory
|-add_memory_resource
 |-add_memory_resource
  |-arch_add_memory
   |-__add_pages
    |-__add_section
     |-sparse_add_one_section
      |-kmalloc_section_memmap
       |-sparse_mem_map_populate
        |-vmemmap_populate
...->arch_add_memory->init_memory_mapping

# Hugetlb page
https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt
[Huge pages part 1 (Introduction)](https://lwn.net/Articles/374424/)
[hugepage 分析](https://ggaaooppeenngg.github.io/zh-CN/2017/04/30/hugepage-%E5%88%86%E6%9E%90/)
## status
p hstates
## Examples
firo@snow /mnt> cat /proc/sys/vm/nr_hugepages
100
firo@snow /mnt> cat /proc/sys/vm/nr_overcommit_hugepages
64

./hugepage-mmap  # from kernel with sleep(100) and /mnt/hugepagefile
Returned address is 0x7f87f4600000 

root@snow:/proc/sys/vm# tail /proc/meminfo 
CmaFree:               0 kB
HugePages_Total:     128
HugePages_Free:      128
HugePages_Rsvd:      128
HugePages_Surp:       28
Hugepagesize:       2048 kB
Hugetlb:          262144 kB
DirectMap4k:      354044 kB
DirectMap2M:    11642880 kB
DirectMap1G:           0 kB

# memmap
 * Only struct pages that are backed by physical memory are zeroed and
 * initialized by going through __init_single_page(). But, there are some
 * struct pages which are reserved in memblock allocator and their fields
 * may be accessed (for example page_to_pfn() on some configuration accesses
 * flags). We must explicitly zero those struct pages.
 * This function also addresses a similar issue where struct pages are left
 * uninitialized because the physical address range is not covered by
 * memblock.memory or memblock.reserved. That could happen when memblock
 * layout is manually configured via memmap=.
void __init zero_resv_unavail(void)

# Memblock and bootmem
// copy e820 to memblock, reconstructs direct memory mapping and setups the direct mapping of the physical memory at PAGE_OFFSET
setup_arch-> memblock_x86_fill
memblock the [implementations](https://0xax.gitbooks.io/linux-insides/content/mm/linux-mm-1.html) of memblock is quite simple. static initialization with variable memblock.
bootmem is discarded by [ARM](https://lkml.org/lkml/2015/12/21/333) and x86 
a little history in e820_register_active_region about memblock [replaced by lmb](https://lkml.org/lkml/2010/7/13/68)
## What is the meaning of the following notes?
### memblock (constantly Y for x86)
memblock_free_late->__memblock_free_late->__free_pages_bootmem
### bootmem (discarded by x86)
memblock_free_late->free_bootmem_late->__free_pages_bootmem
free_all_bootmem->free_all_bootmem_core->__free_pages_bootmem
### nobootmem
free_bootmem_late->__free_pages_bootmem
free_all_bootmem->free_low_memory_core_early->__free_memory_core->*__free_pages_memory*->__free_pages_bootmem->__free_pages_boot_core
### free bootmem core/earyly
mm_init->mem_init->free_all_bootmem
### free bootmem late
start_kernel->efi_free_boot_services->free_bootmem_late->__free_pages_bootmem

# Memory pool
[Memory pools on lwn](https://lwn.net/Articles/22909/)
[Use of mempool in bio](https://lwn.net/Articles/736534/)
tglx: commit 800446073f02f3035bffad7f1ced654ff6b474c9
Author: Linus Torvalds <torvalds@athlon.transmeta.com>
Date:   Mon Feb 4 23:58:50 2002 -0800
    v2.5.0.9 -> v2.5.0.10
    - Jens Axboe: more bio stuff
    - Ingo Molnar: mempool for bio
+++ b/mm/mempool.c
+ *  linux/mm/mempool.c
+ *  memory buffer pool support. Such pools are mostly used to
+ *  guarantee deadlock-free IO operations even during extreme
+ *  VM load.
+ *  started by Ingo Molnar, Copyright (C) 2001
Check mempool_create_node and mempool_alloc_slab for details
